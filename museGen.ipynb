{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "name": "sheet_music_generator2.ipynb",
   "provenance": []
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from music21 import stream, instrument, note, chord\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "\n",
    "\n",
    "try:\n",
    "  import google.colab\n",
    "  IS_ON_GOOGLE_COLAB = True\n",
    "except:\n",
    "  IS_ON_GOOGLE_COLAB = False\n",
    "\n",
    "if IS_ON_GOOGLE_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "id": "S23_wJ0FEFVS",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "142a69dc-ed98-4d37-8b65-87ab0821740b"
   },
   "source": [
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from music21 import converter, pitch, interval, instrument, note, chord\n",
    "import tensorflow as tf\n",
    "# Define save directory\n",
    "from music21.key import Key\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "midi_dir = './midi_songs/'\n",
    "\n",
    "\n",
    "def get_current_datetime():\n",
    "    from datetime import datetime\n",
    "    now = datetime.now()\n",
    "    dt_name = now.strftime(\"%m_%d_%Y__%H_%M_%S\")\n",
    "    return dt_name\n",
    "\n",
    "if IS_ON_GOOGLE_COLAB:\n",
    "  FOLDER_ROOT = \"/content/drive/MyDrive/magisterka/SheetMusicGenerator2\"\n",
    "else:\n",
    "  FOLDER_ROOT = \".\"\n",
    "TEST_RUN = True\n",
    "NORMALIZE_NOTES = True\n",
    "USE_COMPUTED_VALUES = False\n",
    "USE_SAVE_POINT = False\n",
    "\n",
    "NORMALIZATION_BOUNDARIES = [3, 4]\n",
    "EPOCHS = 250\n",
    "LATENT_VECTOR_DIM = 2\n",
    "BATCH_SIZE = 256\n",
    "SEQUENCE_LENGTH = 32\n",
    "# COMPUTED_INT_TO_NOTE_PATH = \"/content/drive/MyDrive/magisterka/SheetMusicGenerator2/AUTOENCODER/data/dicts/int_to_note_08_19_2021__17_25_44\"\n",
    "# COMPUTED_INT_TO_DURATION_PATH = \"/content/drive/MyDrive/magisterka/SheetMusicGenerator2/AUTOENCODER/data/dicts/int_to_duration_08_19_2021__17_25_44\"\n",
    "# COMPUTED_NOTES_PATH = \"/content/drive/MyDrive/magisterka/SheetMusicGenerator2/AUTOENCODER/data/notes/notes_08_19_2021__17_25_44\"\n",
    "# COMPUTED_DURATIONS_PATH = \"/content/drive/MyDrive/magisterka/SheetMusicGenerator2/AUTOENCODER/data/durations/durations_08_19_2021__17_25_44\"\n",
    "COMPUTED_DATA_PATH = \"AUTOENCODER/data/data_file_12_02_2021__18_08_03\"\n",
    "\n",
    "\n",
    "\n",
    "SAVE_POINT = \"AUTOENCODER/checkpoints/08_19_2021__18_34_10/epoch=014-loss=383.5284-acc=0.0000.hdf5\"\n",
    "AUTOENCODER = \"AUTOENCODER\"\n",
    "\n",
    "MODEL_NAME = AUTOENCODER\n",
    "MODEL_FOLDER_ROOT = os.path.join(FOLDER_ROOT, MODEL_NAME)\n",
    "CURR_DT = get_current_datetime()\n",
    "MODEL_DIR_PATH = os.path.join(MODEL_FOLDER_ROOT, \"generated_models\")\n",
    "OCCURENCES = os.path.join(MODEL_FOLDER_ROOT, \"data\", \"occurences\")\n",
    "\n",
    "DATA_DIR = os.path.join(MODEL_FOLDER_ROOT, \"data\")\n",
    "DATA_NOTES_DIR = os.path.join(DATA_DIR, \"notes\")\n",
    "DATA_DURATIONS_DIR = os.path.join(DATA_DIR, \"durations\")\n",
    "\n",
    "DATA_FILE_PATH = os.path.join(DATA_DIR, \"data_file_\" + str(CURR_DT))\n",
    "\n",
    "DATA_DICTS_DIR = os.path.join(DATA_DIR, \"dicts\")\n",
    "DATA_INT_TO_NOTE_PATH = os.path.join(DATA_DICTS_DIR, \"int_to_note_\" + str(CURR_DT))\n",
    "DATA_INT_TO_DURATION_PATH = os.path.join(DATA_DICTS_DIR, \"int_to_duration_\" + str(CURR_DT))\n",
    "DATA_NOTES_PATH = os.path.join(DATA_NOTES_DIR, \"notes_\" + str(CURR_DT))\n",
    "\n",
    "DATA_DURATIONS_PATH = os.path.join(DATA_DURATIONS_DIR, \"durations_\" + str(CURR_DT))\n",
    "# MIDI_SONGS_DIR = os.path.join(FOLDER_ROOT, \"midi_songs\")\n",
    "MIDI_SONGS_DIR = os.path.join(FOLDER_ROOT, \"midi_songs_smaller\")\n",
    "# MIDI_SONGS_DIR = os.path.join(FOLDER_ROOT, \"midi_songs_medium\")\n",
    "MIDI_GENERATED_DIR = os.path.join(MODEL_FOLDER_ROOT, \"midi_generated\")\n",
    "MIDI_SONGS_REGEX = os.path.join(MIDI_SONGS_DIR, \"*.mid\")\n",
    "CHECKPOINTS_DIR = os.path.join(MODEL_FOLDER_ROOT, \"checkpoints\")\n",
    "CHECKPOINT = os.path.join(CHECKPOINTS_DIR, str(CURR_DT))\n",
    "LOGS_DIR = os.path.join(MODEL_FOLDER_ROOT, \"logs\")\n",
    "\n",
    "LOG = os.path.join(LOGS_DIR, str(CURR_DT))\n",
    "all_paths = [MODEL_DIR_PATH, OCCURENCES, DATA_NOTES_DIR, DATA_DURATIONS_DIR, DATA_DICTS_DIR,\n",
    "             MIDI_GENERATED_DIR, CHECKPOINTS_DIR, CHECKPOINT, LOGS_DIR, LOG]\n",
    "\n",
    "for path in all_paths:\n",
    "    Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# def create_train_data():\n",
    "# # Create empty list for scores\n",
    "#     originalScores = []\n",
    "#\n",
    "#     # Load and make list of stream objects\n",
    "#     for indx, song in enumerate(glob.glob(MIDI_SONGS_REGEX)):\n",
    "#\n",
    "#         print(\"Parsing song: \" + str(song))\n",
    "#         score = converter.parse(song)\n",
    "#         originalScores.append(score)\n",
    "#         if TEST_RUN and indx == 1:\n",
    "#             break\n",
    "#\n",
    "#     # Define empty lists of lists\n",
    "#     originalChords = [[] for _ in originalScores]\n",
    "#     originalDurations = [[] for _ in originalScores]\n",
    "#     originalKeys = []\n",
    "#\n",
    "#     def transpose_amount(score):\n",
    "#         return -int(score.chordify().analyze('key').tonic.ps % 12)\n",
    "#\n",
    "#     def monophonic(stream):\n",
    "#         try:\n",
    "#             length = len(instrument.partitionByInstrument(stream).parts)\n",
    "#         except:\n",
    "#             length = 0\n",
    "#         return length == 1\n",
    "#     # Extract notes, chords, durations, and keys\n",
    "#\n",
    "#\n",
    "#     originalScores = [song.chordify() for song in originalScores]\n",
    "#\n",
    "#     for i, song in enumerate(originalScores):\n",
    "#         originalKeys.append(str(song.analyze('key')))\n",
    "#         # song.transpose\n",
    "#         transp_int = transpose_amount(song)\n",
    "#         for element in song:\n",
    "#             if isinstance(element, note.Note):\n",
    "#                 originalChords[i].append(element.pitch.transpose(transp_int))\n",
    "#                 originalDurations[i].append(element.duration.quarterLength)\n",
    "#             elif isinstance(element, chord.Chord):\n",
    "#                 originalChords[i].append('.'.join(str(n.transpose(transp_int)) for n in element.pitches))\n",
    "#                 originalDurations[i].append(element.duration.quarterLength)\n",
    "#         print(str(i))\n",
    "#\n",
    "#     cChords = [c for (c, k) in zip(originalChords, originalKeys) if (k == 'C major')]\n",
    "#     cDurations = [c for (c, k) in zip(originalDurations, originalKeys) if (k == 'C major')]\n",
    "#     # Map unique chords to integers\n",
    "#     uniqueChords = np.unique([i for s in originalChords for i in s])\n",
    "#     chordToInt = dict(zip(uniqueChords, list(range(0, len(uniqueChords)))))\n",
    "#\n",
    "#     # Map unique durations to integers\n",
    "#     uniqueDurations = np.unique([i for s in originalDurations for i in s])\n",
    "#     durationToInt = dict(zip(uniqueDurations, list(range(0, len(uniqueDurations)))))\n",
    "#\n",
    "#     # Print number of unique notes and chords\n",
    "#     print(len(uniqueChords))\n",
    "#\n",
    "#     # Print number of unique durations\n",
    "#     print(len(uniqueDurations))\n",
    "#\n",
    "#     intToChord = {i: c for c, i in chordToInt.items()}\n",
    "#     intToDuration = {i: c for c, i in durationToInt.items()}\n",
    "#\n",
    "#     # Define sequence length\n",
    "#     sequenceLength = 32\n",
    "#\n",
    "#     # Define empty arrays for train data\n",
    "#     trainChords = []\n",
    "#     trainDurations = []\n",
    "#\n",
    "#     # Construct training sequences for chords and durations\n",
    "#     for s in range(len(cChords)):\n",
    "#         chordList = [chordToInt[c] for c in cChords[s]]\n",
    "#         durationList = [durationToInt[d] for d in cDurations[s]]\n",
    "#         for i in range(len(chordList) - sequenceLength):\n",
    "#             trainChords.append(chordList[i:i+sequenceLength])\n",
    "#             trainDurations.append(durationList[i:i+sequenceLength])\n",
    "#\n",
    "#     with open(DATA_NOTES_DIR + \"/notes\", 'wb') as filepath:\n",
    "#         pickle.dump(cChords, filepath)\n",
    "#\n",
    "#     with open(DATA_DURATIONS_DIR + \"/durations\", 'wb') as filepath:\n",
    "#         pickle.dump(cDurations, filepath)\n",
    "# #\n",
    "# #     trainChords = tf.keras.utils.to_categorical(trainChords).transpose(0,2,1)\n",
    "# #\n",
    "# # # Convert data to numpy array of type float\n",
    "# #     trainChords = np.array(trainChords, np.float)\n",
    "#\n",
    "# # Flatten sequence of chords into single dimension\n",
    "#         # Convert to one-hot encoding and swap chord and sequence dimensions\n",
    "#     trainChords = tf.keras.utils.to_categorical(trainChords).transpose(0, 2, 1)\n",
    "#\n",
    "#     # Convert data to numpy array of type float\n",
    "#     trainChords = np.array(trainChords, np.float)\n",
    "#\n",
    "#     nSamples = trainChords.shape[0]\n",
    "#     nChords = trainChords.shape[1]\n",
    "#     inputDim = nChords * sequenceLength\n",
    "#     # Flatten sequence of chords into single dimension\n",
    "#     trainChordsFlat = trainChords.reshape(nSamples, inputDim)\n",
    "#\n",
    "#     return trainChordsFlat, inputDim, trainDurations, sequenceLength, intToChord, intToDuration, nChords\n",
    "# if __name__ == \"__main__\":\n",
    "#     create_train_data()\n",
    "# # Convert to one-hot encoding and swap chord and sequence dimensions"
   ],
   "execution_count": 28,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "b1JArktFEFVY"
   },
   "source": [
    "class Trainer():\n",
    "    def __init__(self, model, x, y):\n",
    "        self.model = model\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.steps_per_epoch = len(x)\n",
    "\n",
    "    def train(self, checkpoint_path=None):\n",
    "        # Define number of samples, chords and notes, and input dimension\n",
    "        # filepath = CHECKPOINTS + \"weights-improvement-{epoch:02d}-{loss:.4f}-{categorical_accuracy:.4f}-bigger.hdf5\"\n",
    "        # filepath = \"weights-improvement-epoch:{epoch:02d}-loss:{loss:.4f}-cat_acc:{categorical_accuracy:.4f}.hdf5\"\n",
    "        if checkpoint_path:\n",
    "            self.model.load_weights(checkpoint_path)\n",
    "            nb_epoch = int(os.path.basename(checkpoint_path).split(\"=\")[1].split(\"-\")[0])\n",
    "\n",
    "        else:\n",
    "            nb_epoch = 0\n",
    "        filepath = os.path.join(CHECKPOINT, \"epoch={epoch:03d}-loss={loss:.4f}-acc={binary_crossentropy:.4f}.hdf5\")\n",
    "\n",
    "        # filepath = \"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
    "        checkpoint = ModelCheckpoint(\n",
    "            filepath,\n",
    "            monitor='loss',\n",
    "            verbose=0,\n",
    "            save_best_only=True,\n",
    "            mode='min'\n",
    "        )\n",
    "        log = tf.keras.callbacks.TensorBoard(log_dir=LOG + \" \" + str(self.model))\n",
    "\n",
    "\n",
    "        callbacks_list = [checkpoint, log]\n",
    "        # history = self.model.fit(network_input, network_output, epochs=EPOCHS, batch_size=128, callbacks=callbacks_list)\n",
    "        # model.save(MODEL_DIR_PATH + MODEL_NAME + \"_\" + CURR_DT + \".hdf5\")\n",
    "\n",
    "        self.model.compile(loss='binary_crossentropy', optimizer=RMSprop(learning_rate=0.001),\n",
    "                           metrics=[tf.keras.losses.BinaryCrossentropy()])\n",
    "        # self.model.compile(loss='binary_crossentropy', optimizer=RMSprop(learning_rate=0.01), metrics=[\"accuracy\"])\n",
    "        # Train autoencoder\n",
    "        self.model.summary()\n",
    "        print(MODEL_DIR_PATH + MODEL_NAME + \"_\" + CURR_DT + \".hdf5\")\n",
    "        # history = self.model.fit(self.trainNotesFlat, self.trainNotesFlat, epochs=1)\n",
    "        # history = self.model.fit(self.trainNotesFlat, self.trainNotesFlat, epochs=500, callbacks=callbacks_list, batch_size=8)\n",
    "        # tensor_dataset = tf.data.Dataset.from_tensors((self.trainNotesFlat, self.trainNotesFlat))\n",
    "\n",
    "        # history = self.model.fit(x=self.data_generator(),\n",
    "        #                          epochs=EPOCHS,\n",
    "        #                          callbacks=callbacks_list,\n",
    "        #                          batch_size=BATCH_SIZE,\n",
    "        #                          steps_per_epoch=self.steps_per_epoch)\n",
    "        # history = self.model.fit_generator(self.data_generator(),\n",
    "        #                          epochs=EPOCHS,\n",
    "        #                          callbacks=callbacks_list,\n",
    "        #                          initial_epoch=nb_epoch,\n",
    "        #                          # batch_size=BATCH_SIZE,\n",
    "        #                          steps_per_epoch=self.steps_per_epoch)\n",
    "\n",
    "        history = self.model.fit(x=self.x,\n",
    "                                 y=self.y,\n",
    "                         epochs=EPOCHS,\n",
    "                         callbacks=callbacks_list,\n",
    "                         initial_epoch=nb_epoch,\n",
    "                         # batch_size=BATCH_SIZE,\n",
    "                         steps_per_epoch=self.steps_per_epoch)\n",
    "        print(history.history)\n",
    "        print(MODEL_DIR_PATH + MODEL_NAME + \"_\" + CURR_DT + \".hdf5\")\n",
    "        self.model.save(os.path.join(MODEL_DIR_PATH, MODEL_NAME + \"_\" + CURR_DT + \".hdf5\"))"
   ],
   "execution_count": 29,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "class PreparedData:\n",
    "    def __init__(self):\n",
    "        self.train_notes = None\n",
    "        self.train_durations = None\n",
    "        self.int_to_note = None\n",
    "        self.int_to_duration = None\n",
    "        self.sequence_length = None\n",
    "\n",
    "        self.n_of_unique_notes_classes = None\n",
    "        self.input_dim = None\n",
    "        self.dataset = None\n",
    "\n",
    "    def load_prepared_data(self):\n",
    "        with open(COMPUTED_DATA_PATH, 'rb') as data_file:\n",
    "            self.train_notes, self.train_durations, self.int_to_note, self.int_to_duration, self.sequence_length = pickle.load(data_file)\n",
    "\n",
    "    def save_values(self):\n",
    "        with open(DATA_FILE_PATH, 'wb') as filepath:\n",
    "            pickle.dump([self.train_notes, self.train_durations, self.int_to_note, self.int_to_duration, self.sequence_length], filepath)\n",
    "\n",
    "        # with open(DATA_DURATIONS_PATH, 'rb') as train_durations_file:\n",
    "        #     self.train_durations = pickle.load(train_durations_file)\n",
    "        #\n",
    "        # with open(DATA_INT_TO_NOTE_PATH, 'rb') as int_to_note_file:\n",
    "        #     self.int_to_note = pickle.load(int_to_note_file)\n",
    "        #\n",
    "        # with open(DATA_INT_TO_DURATION_PATH, 'rb') as int_to_duration_file:\n",
    "        #     self.int_to_duration = pickle.load(int_to_duration_file)\n",
    "\n",
    "    def parse_songs(self, sequence_length):\n",
    "        self.sequence_length = sequence_length\n",
    "        # Create empty list for scores\n",
    "        original_scores = []\n",
    "\n",
    "        # Load and make list of stream objects\n",
    "        for indx, song in enumerate(glob.glob(MIDI_SONGS_REGEX)):\n",
    "            print(\"Parsing song: \" + str(song))\n",
    "            score = converter.parse(song)\n",
    "            original_scores.append(score)\n",
    "            if TEST_RUN and indx == 1:\n",
    "                break\n",
    "\n",
    "        # Define empty lists of lists\n",
    "        original_notes = [[] for _ in original_scores]\n",
    "        original_durations = [[] for _ in original_scores]\n",
    "        original_keys = []\n",
    "\n",
    "        def transpose_amount(score):\n",
    "            return -int(score.chordify().analyze('key').tonic.ps % 12)\n",
    "\n",
    "        def monophonic(stream):\n",
    "            try:\n",
    "                length = len(instrument.partitionByInstrument(stream).parts)\n",
    "\n",
    "            except:\n",
    "                length = 0\n",
    "            return length == 1\n",
    "\n",
    "        # Extract notes, notes, durations, and keys\n",
    "\n",
    "        original_scores = [song.chordify() for song in original_scores]\n",
    "\n",
    "        for i, song in enumerate(original_scores):\n",
    "            transp_int = transpose_amount(song)\n",
    "            original_keys.append(str(song.analyze('key').transpose(transp_int)))\n",
    "            for element in song:\n",
    "                if isinstance(element, note.Note):\n",
    "                    original_notes[i].append(element.pitch.transpose(transp_int))\n",
    "                    original_durations[i].append(element.duration.quarterLength)\n",
    "\n",
    "                elif isinstance(element, chord.Chord):\n",
    "                    original_notes[i].append('.'.join(str(n.transpose(transp_int)) for n in element.pitches))\n",
    "                    original_durations[i].append(element.duration.quarterLength)\n",
    "            print(str(original_keys[i]))\n",
    "\n",
    "        c_notes = [c for (c, k) in zip(original_notes, original_keys) if (k == 'C major')]\n",
    "        c_durations = [c for (c, k) in zip(original_durations, original_keys) if (k == 'C major')]\n",
    "        # Map unique notes to integers\n",
    "        unique_notes = np.unique([i for s in original_notes for i in s])\n",
    "        note_to_int = dict(zip(unique_notes, list(range(0, len(unique_notes)))))\n",
    "\n",
    "        # Map unique durations to integers\n",
    "        unique_durations = np.unique([i for s in original_durations for i in s])\n",
    "        duration_to_int = dict(zip(unique_durations, list(range(0, len(unique_durations)))))\n",
    "\n",
    "        # Print number of unique notes and notes\n",
    "        print(len(unique_notes))\n",
    "\n",
    "        # Print number of unique durations\n",
    "        print(len(unique_durations))\n",
    "\n",
    "        int_to_note = {i: c for c, i in note_to_int.items()}\n",
    "        int_to_duration = {i: c for c, i in duration_to_int.items()}\n",
    "\n",
    "        # Define sequence length\n",
    "\n",
    "        # Define empty arrays for train data\n",
    "        train_notes = []\n",
    "        train_durations = []\n",
    "\n",
    "        # Construct training sequences for notes and durations\n",
    "        for s in range(len(c_notes)):\n",
    "            note_list = [note_to_int[c] for c in c_notes[s]]\n",
    "            duration_list = [duration_to_int[d] for d in c_durations[s]]\n",
    "            for i in range(len(note_list) - sequence_length):\n",
    "                train_notes.append(note_list[i:i + sequence_length])\n",
    "                train_durations.append(duration_list[i:i + sequence_length])\n",
    "\n",
    "        self.train_notes = np.array(train_notes)\n",
    "        self.train_durations = np.array(train_durations)\n",
    "        self.int_to_note = np.array(int_to_note)\n",
    "        self.int_to_duration = np.array(int_to_duration)\n",
    "\n",
    "        # return train_notes, train_durations, int_to_note, int_to_duration\n",
    "\n",
    "        # with open(DATA_NOTES_PATH, 'wb') as filepath:\n",
    "        #     pickle.dump(self.train_notes, filepath)\n",
    "        #\n",
    "        # with open(DATA_DURATIONS_PATH, 'wb') as filepath:\n",
    "        #     pickle.dump(self.train_durations, filepath)\n",
    "        #\n",
    "        # with open(DATA_INT_TO_NOTE_PATH, 'wb') as filepath:\n",
    "        #     pickle.dump(self.int_to_note, filepath)\n",
    "        #\n",
    "        # with open(DATA_INT_TO_DURATION_PATH, 'wb') as filepath:\n",
    "        #     pickle.dump(self.int_to_duration, filepath)\n",
    "        # self.train_notes, = train_notes\n",
    "        # self.train_durations, = train_durations\n",
    "        # self.int_to_note, = int_to_note\n",
    "        # self.int_to_duration, = int_to_duration\n",
    "\n",
    "    # @staticmethod\n",
    "    def prepare_data(self, sequence_length):\n",
    "        # print(\"trainNotesFlat: \" + str(train_notes))\n",
    "\n",
    "        # train_notes_categorical_not_transposed = tf.keras.utils.to_categorical(self.train_notes, dtype=\"float16\")\n",
    "        # Convert data to numpy array of type float\n",
    "        # trainNotes = np.array(trainNotes, np.float32)\n",
    "        train_notes_categorical = tf.keras.utils.to_categorical(self.train_notes, dtype=\"float16\")  # .transpose(0, 2, 1)\n",
    "\n",
    "        # print(f\"train_notes_categorical.device: {train_notes_categorical.device}\")\n",
    "        n_samples = train_notes_categorical.shape[0]\n",
    "        self.n_of_unique_notes_classes = train_notes_categorical.shape[2]\n",
    "        self.input_dim = self.n_of_unique_notes_classes * sequence_length\n",
    "        # Flatten sequence of notes into single dimension\n",
    "        # train_notes_flattened = train_notes_categorical.reshape(n_samples, self.input_dim)\n",
    "        self.dataset = train_notes_categorical.reshape(-1, self.input_dim)\n",
    "        print(\"type(train_notes_flattened): \" + str(type(self.dataset)))\n",
    "        # self.numpy_dataset = train_notes_flattened\n",
    "        return self.input_dim, self.dataset, self.n_of_unique_notes_classes\n",
    "\n",
    "\n",
    "class MusicAutoencoder(tf.keras.Model):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(MusicAutoencoder, self).__init__()\n",
    "        # self.prepared_data = prepared_data\n",
    "        self.latent_dim = latent_dim\n",
    "        # self.sequence_length = sequence_length\n",
    "        self.tensor_dataset = None\n",
    "        self.input_dim = None\n",
    "        self.n_of_unique_notes_classes = None\n",
    "        self.numpy_dataset = None\n",
    "        # self.train_notes = prepared_data.train_notes\n",
    "        # self.train_notes_path = train_notes_path\n",
    "        # self.train_durations_path = train_durations_path\n",
    "        # self.int_to_note_path = int_to_note_path\n",
    "        # self.int_to_duration_path = int_to_duration_path\n",
    "\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "                Input(shape=SEQUENCE_LENGTH, batch_size=BATCH_SIZE),\n",
    "                Dense(self.latent_dim, activation='tanh')\n",
    "            ])\n",
    "\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "                Input(shape=self.latent_dim),\n",
    "                Dense(SEQUENCE_LENGTH, activation='sigmoid')\n",
    "            ])\n",
    "        # self.encoder = tf.keras.Sequential([\n",
    "        #     tf.keras.layers.Input(shape = input_dim),\n",
    "        #     tf.keras.layers.Dense(input_dim, activation = 'tanh')\n",
    "        # ])\n",
    "        #\n",
    "        # self.decoder = tf.keras.Sequential([\n",
    "        #     tf.keras.layers.Input(shape =input_dim),\n",
    "        #     tf.keras.layers.Dense(input_dim, activation = 'sigmoid')\n",
    "        # ])\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # encoderInput = tf.keras.layers.Input(shape = self.inputDim)\n",
    "        # latent = tf.keras.layers.Input(shape = self.latentDim)\n",
    "        encoded = self.encoder(inputs)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "    # def autoencoder(self, inputDim, latentDim):\n",
    "    #     # Define encoder input shape\n",
    "    #     encoderInput = tf.keras.layers.Input(shape =inputDim)\n",
    "    #\n",
    "    #     # Define decoder input shape\n",
    "    #     latent = tf.keras.layers.Input(shape =latentDim)\n",
    "    #\n",
    "    #     # Define dense encoding layer connecting input to latent vector\n",
    "    #     encoded = tf.keras.layers.Dense(latentDim, activation = 'tanh')(encoderInput)\n",
    "    #\n",
    "    #     # Define dense decoding layer connecting latent vector to output\n",
    "    #     decoded = tf.keras.layers.Dense(inputDim, activation = 'sigmoid')(latent)\n",
    "    #\n",
    "    #     # Define the encoder and decoder models\n",
    "    #     self.encoder = tf.keras.Model(encoderInput, encoded)\n",
    "    #     self.decoder = tf.keras.Model(latent, decoded)\n",
    "    #\n",
    "    #     # Define autoencoder model\n",
    "    #     autoencoder = tf.keras.Model(encoderInput, self.decoder(encoded))\n",
    "    #     return autoencoder\n",
    "\n",
    "\n",
    "\n",
    "    def train(self, prepared_data, checkpoint_path=None):\n",
    "        # Define number of samples, chords and notes, and input dimension\n",
    "        # filepath = CHECKPOINTS + \"weights-improvement-{epoch:02d}-{loss:.4f}-{categorical_accuracy:.4f}-bigger.hdf5\"\n",
    "        # filepath = \"weights-improvement-epoch:{epoch:02d}-loss:{loss:.4f}-cat_acc:{categorical_accuracy:.4f}.hdf5\"\n",
    "        curr_dt = get_current_datetime()\n",
    "        print(str(\"Current datatime: \" + curr_dt))\n",
    "\n",
    "        if checkpoint_path:\n",
    "            self.model.load_weights(checkpoint_path)\n",
    "\n",
    "        filepath = CHECKPOINT + str(curr_dt) + \"/\" + \"epoch:{epoch:02d}-loss:{loss:.4f}-acc:{binary_accuracy:.4f}.hdf5\"\n",
    "\n",
    "        # filepath = \"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
    "        checkpoint = ModelCheckpoint(\n",
    "            filepath,\n",
    "            monitor='binary_accuracy',\n",
    "            verbose=0,\n",
    "            save_best_only=True,\n",
    "            mode='max'\n",
    "        )\n",
    "        log = tf.keras.callbacks.TensorBoard(log_dir=LOG + curr_dt),\n",
    "\n",
    "        callbacks_list = [checkpoint, log]\n",
    "        # history = self.model.fit(network_input, network_output, epochs=EPOCHS, batch_size=128, callbacks=callbacks_list)\n",
    "        # model.save(MODEL_DIR_PATH + MODEL_NAME + \"_\" + curr_dt + \".hdf5\")\n",
    "\n",
    "        self.compile(loss='binary_crossentropy', optimizer=RMSprop(learning_rate=0.01), metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
    "        # self.summary()\n",
    "        # self.model.compile(loss='binary_crossentropy', optimizer=RMSprop(learning_rate=0.01), metrics=[\"accuracy\"])\n",
    "        # Train autoencoder\n",
    "\n",
    "        print(MODEL_DIR_PATH + MODEL_NAME + \"_\" + curr_dt + \".hdf5\")\n",
    "        # history = self.model.fit(self.trainChordsFlat, self.trainChordsFlat, epochs=1)\n",
    "        # history = self.model.fit(self.trainChordsFlat, self.trainChordsFlat, epochs=500, callbacks=callbacks_list)\n",
    "        print(str(prepared_data.dataset.shape))\n",
    "        # print(str(self.prepared_data.shape))\n",
    "        history = self.fit(prepared_data.dataset, prepared_data.dataset, epochs=EPOCHS, callbacks=callbacks_list)\n",
    "        print(history.history)\n",
    "        print(MODEL_DIR_PATH + MODEL_NAME + \"_\" + curr_dt + \".hdf5\")\n",
    "        self.save(MODEL_DIR_PATH + MODEL_NAME + \"_\" + curr_dt + \".hdf5\")\n",
    "\n",
    "\n",
    "    def generate_notes(self):\n",
    "        generated_notes = self.decoder(np.random.normal(size=(1, self.latent_dim)))\\\n",
    "            .numpy().reshape(self.n_of_unique_notes_classes, self.sequence_length)\\\n",
    "            .argmax(0)\n",
    "\n",
    "        generated_stream = stream.Stream()\n",
    "        generated_stream.append(instrument.Piano())\n",
    "        note_sequence = [self.int_to_note[c] for c in generated_notes]\n",
    "        # Append notes and notes to stream object\n",
    "        for j in range(len(note_sequence)):\n",
    "            try:\n",
    "                generated_stream.append(note.Note(note_sequence[j].replace('.', ' ')))\n",
    "            except:\n",
    "                generated_stream.append(note.Note(note_sequence[j].replace('.', ' ')))\n",
    "\n",
    "        generated_stream.write('midi', fp=MIDI_GENERATED_DIR + 'autoencoder.mid')\n",
    "\n",
    "\n",
    "\n",
    "class ModelFactory:\n",
    "    @staticmethod\n",
    "    def create_model(model_type):\n",
    "        if model_type == AUTOENCODER:\n",
    "            model = MusicAutoencoder(LATENT_VECTOR_DIM)\n",
    "            return model\n",
    "\n",
    "    @staticmethod\n",
    "    def create_prepared_data(use_computed_values):\n",
    "        prepared_data = PreparedData()\n",
    "        if use_computed_values:\n",
    "            prepared_data.load_prepared_data()\n",
    "        else:\n",
    "            prepared_data.parse_songs(SEQUENCE_LENGTH)\n",
    "            prepared_data.save_values()\n",
    "\n",
    "        prepared_data.prepare_data(SEQUENCE_LENGTH)\n",
    "        return prepared_data\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "NCWrUciWEFVc",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "outputId": "61686263-885c-47a4-a81e-c14549752e58"
   },
   "source": [
    "# modelFactory = ModelFactory()\n",
    "prepared_data = ModelFactory.create_prepared_data(USE_COMPUTED_VALUES)\n",
    "musicAutoEncoder = ModelFactory.create_model(AUTOENCODER)\n",
    "musicAutoEncoder.train(prepared_data)\n",
    "# musicAutoEncoder.summary()\n",
    "musicAutoEncoder.generateChords()"
   ],
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing song: ./midi_songs_smaller/caitsith.mid\n",
      "Parsing song: ./midi_songs_smaller/decisive.mid\n",
      "C major\n",
      "c minor\n",
      "374\n",
      "10\n",
      "type(train_notes_flattened): <class 'numpy.ndarray'>\n",
      "Current datatime: 12_06_2021__16_22_52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-06 16:22:52.333317: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2021-12-06 16:22:52.333367: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "2021-12-06 16:22:52.333741: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./AUTOENCODER/generated_modelsAUTOENCODER_12_06_2021__16_22_52.hdf5\n",
      "(888, 11936)\n",
      "Epoch 1/250\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /home/nexon/OneDrive/Studia/Praca_magisterska/Projekt/SheetMusicGenerator2/venv/lib/python3.7/site-packages/keras/engine/training.py:853 train_function  *\n        return step_function(self, iterator)\n    /tmp/ipykernel_18496/3540579809.py:192 call  *\n        encoded = self.encoder(inputs)\n    /home/nexon/OneDrive/Studia/Praca_magisterska/Projekt/SheetMusicGenerator2/venv/lib/python3.7/site-packages/keras/engine/base_layer.py:1020 __call__  **\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /home/nexon/OneDrive/Studia/Praca_magisterska/Projekt/SheetMusicGenerator2/venv/lib/python3.7/site-packages/keras/engine/input_spec.py:254 assert_input_compatibility\n        ' but received input with shape ' + display_shape(x.shape))\n\n    ValueError: Input 0 of layer sequential_6 is incompatible with the layer: expected axis -1 of input shape to have value 32 but received input with shape (None, 11936)\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_18496/726960340.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mprepared_data\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mModelFactory\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcreate_prepared_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mUSE_COMPUTED_VALUES\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mmusicAutoEncoder\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mModelFactory\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcreate_model\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mAUTOENCODER\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m \u001B[0mmusicAutoEncoder\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprepared_data\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      6\u001B[0m \u001B[0;31m# musicAutoEncoder.summary()\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0mmusicAutoEncoder\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgenerateChords\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_18496/633635108.py\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(self, prepared_data, checkpoint_path)\u001B[0m\n\u001B[1;32m    253\u001B[0m         \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprepared_data\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    254\u001B[0m         \u001B[0;31m# print(str(self.prepared_data.shape))\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 255\u001B[0;31m         \u001B[0mhistory\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprepared_data\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mprepared_data\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mepochs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mEPOCHS\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcallbacks\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcallbacks_list\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    256\u001B[0m         \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mhistory\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhistory\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    257\u001B[0m         \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mMODEL_DIR_PATH\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mMODEL_NAME\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m\"_\"\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mcurr_dt\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m\".hdf5\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/OneDrive/Studia/Praca_magisterska/Projekt/SheetMusicGenerator2/venv/lib/python3.7/site-packages/keras/engine/training.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[1;32m   1182\u001B[0m                 _r=1):\n\u001B[1;32m   1183\u001B[0m               \u001B[0mcallbacks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_train_batch_begin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1184\u001B[0;31m               \u001B[0mtmp_logs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0miterator\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1185\u001B[0m               \u001B[0;32mif\u001B[0m \u001B[0mdata_handler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshould_sync\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1186\u001B[0m                 \u001B[0mcontext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0masync_wait\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/OneDrive/Studia/Praca_magisterska/Projekt/SheetMusicGenerator2/venv/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    883\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    884\u001B[0m       \u001B[0;32mwith\u001B[0m \u001B[0mOptionalXlaContext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jit_compile\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 885\u001B[0;31m         \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    886\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    887\u001B[0m       \u001B[0mnew_tracing_count\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexperimental_get_tracing_count\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/OneDrive/Studia/Praca_magisterska/Projekt/SheetMusicGenerator2/venv/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001B[0m in \u001B[0;36m_call\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    931\u001B[0m       \u001B[0;31m# This is the first call of __call__, so we have to initialize.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    932\u001B[0m       \u001B[0minitializers\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 933\u001B[0;31m       \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_initialize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0madd_initializers_to\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0minitializers\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    934\u001B[0m     \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    935\u001B[0m       \u001B[0;31m# At this point we know that the initialization is complete (or less\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/OneDrive/Studia/Praca_magisterska/Projekt/SheetMusicGenerator2/venv/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001B[0m in \u001B[0;36m_initialize\u001B[0;34m(self, args, kwds, add_initializers_to)\u001B[0m\n\u001B[1;32m    758\u001B[0m     self._concrete_stateful_fn = (\n\u001B[1;32m    759\u001B[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001B[0;32m--> 760\u001B[0;31m             *args, **kwds))\n\u001B[0m\u001B[1;32m    761\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    762\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0minvalid_creator_scope\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0munused_args\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0munused_kwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/OneDrive/Studia/Praca_magisterska/Projekt/SheetMusicGenerator2/venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36m_get_concrete_function_internal_garbage_collected\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   3064\u001B[0m       \u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3065\u001B[0m     \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_lock\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 3066\u001B[0;31m       \u001B[0mgraph_function\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_maybe_define_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   3067\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mgraph_function\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3068\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/OneDrive/Studia/Praca_magisterska/Projekt/SheetMusicGenerator2/venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36m_maybe_define_function\u001B[0;34m(self, args, kwargs)\u001B[0m\n\u001B[1;32m   3461\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3462\u001B[0m           \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_function_cache\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmissed\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcall_context_key\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 3463\u001B[0;31m           \u001B[0mgraph_function\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_create_graph_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   3464\u001B[0m           \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_function_cache\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprimary\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mcache_key\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mgraph_function\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3465\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/OneDrive/Studia/Praca_magisterska/Projekt/SheetMusicGenerator2/venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36m_create_graph_function\u001B[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001B[0m\n\u001B[1;32m   3306\u001B[0m             \u001B[0marg_names\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0marg_names\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3307\u001B[0m             \u001B[0moverride_flat_arg_shapes\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0moverride_flat_arg_shapes\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 3308\u001B[0;31m             capture_by_value=self._capture_by_value),\n\u001B[0m\u001B[1;32m   3309\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_function_attributes\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3310\u001B[0m         \u001B[0mfunction_spec\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfunction_spec\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/OneDrive/Studia/Praca_magisterska/Projekt/SheetMusicGenerator2/venv/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001B[0m in \u001B[0;36mfunc_graph_from_py_func\u001B[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001B[0m\n\u001B[1;32m   1005\u001B[0m         \u001B[0m_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moriginal_func\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf_decorator\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0munwrap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpython_func\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1006\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1007\u001B[0;31m       \u001B[0mfunc_outputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpython_func\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mfunc_args\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mfunc_kwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1008\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1009\u001B[0m       \u001B[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/OneDrive/Studia/Praca_magisterska/Projekt/SheetMusicGenerator2/venv/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001B[0m in \u001B[0;36mwrapped_fn\u001B[0;34m(*args, **kwds)\u001B[0m\n\u001B[1;32m    666\u001B[0m         \u001B[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    667\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mOptionalXlaContext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcompile_with_xla\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 668\u001B[0;31m           \u001B[0mout\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mweak_wrapped_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__wrapped__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    669\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mout\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    670\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/OneDrive/Studia/Praca_magisterska/Projekt/SheetMusicGenerator2/venv/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    992\u001B[0m           \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m  \u001B[0;31m# pylint:disable=broad-except\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    993\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"ag_error_metadata\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 994\u001B[0;31m               \u001B[0;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mag_error_metadata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto_exception\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    995\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    996\u001B[0m               \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: in user code:\n\n    /home/nexon/OneDrive/Studia/Praca_magisterska/Projekt/SheetMusicGenerator2/venv/lib/python3.7/site-packages/keras/engine/training.py:853 train_function  *\n        return step_function(self, iterator)\n    /tmp/ipykernel_18496/3540579809.py:192 call  *\n        encoded = self.encoder(inputs)\n    /home/nexon/OneDrive/Studia/Praca_magisterska/Projekt/SheetMusicGenerator2/venv/lib/python3.7/site-packages/keras/engine/base_layer.py:1020 __call__  **\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /home/nexon/OneDrive/Studia/Praca_magisterska/Projekt/SheetMusicGenerator2/venv/lib/python3.7/site-packages/keras/engine/input_spec.py:254 assert_input_compatibility\n        ' but received input with shape ' + display_shape(x.shape))\n\n    ValueError: Input 0 of layer sequential_6 is incompatible with the layer: expected axis -1 of input shape to have value 32 but received input with shape (None, 11936)\n"
     ]
    }
   ]
  }
 ]
}